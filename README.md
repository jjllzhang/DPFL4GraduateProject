# ğŸ’  Differential Privacy and Shuffler in Federated Learning System

This graduate project aims to enhance the privacy protection capabilities of federated learning systems by integrating Differential Privacy and the Shuffler mechanism, designed for distributed machine learning scenarios with stringent privacy requirements.

## ğŸŒ“ Motivation and Background

Protecting individual privacy in the era of ubiquitous data is a significant challenge. Federated learning offers a way to conduct machine learning tasks without sharing raw data among participants but still faces potential privacy leakage risks. This project introduces Differential Privacy and the Shuffler mechanism to minimize these risks and enhance the system's overall privacy protection.

## ğŸ’… Folder Directory Structure

The folder directory structure is organized as follows:

```bash
.
â”œâ”€â”€ algorithms  ## Houses the implementation of the Differential Privacy Stochastic Gradient Descent (DPSGD) algorithm and the training script utilizing DPSGD.
â”‚Â Â  â”œâ”€â”€ DPSGD.py
â”‚Â Â  â””â”€â”€ train_with_DPSGD.py
â”œâ”€â”€ data        ## Contains datasets (e.g., CIFAR-10, Fashion MNIST, MNIST) and utility scripts for data handling and processing.
â”‚Â Â  â”œâ”€â”€ cifar-10-batches-py
â”‚Â Â  â”œâ”€â”€ FashionMNIST
â”‚Â Â  â”œâ”€â”€ MNIST
â”‚Â Â  â””â”€â”€ utils
â”‚Â Â      â”œâ”€â”€ custom_tensor_dataset.py
â”‚Â Â      â”œâ”€â”€ dirichlet_nonIID_data.py
â”‚Â Â      â”œâ”€â”€ get_data.py
â”‚Â Â      â””â”€â”€ sampling.py
â”œâ”€â”€ FL          ## Dedicated to Federated Learning (FL) scripts, including various federated averaging implementations with differential privacy integrated.
â”‚Â Â  â”œâ”€â”€ fed_avg
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ fed_avg.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ fed_avg_with_dp_auto.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ fed_avg_with_dp_perlayer.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ fed_avg_with_dp.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ fed_avg_with_dp_with_shuffler.py
â”‚Â Â  â””â”€â”€ utils
â”‚Â Â      â”œâ”€â”€ create_client.py
â”‚Â Â      â”œâ”€â”€ local_train.py
â”‚Â Â      â”œâ”€â”€ log_helper.py
â”‚Â Â      â”œâ”€â”€ train_helper.py
â”‚Â Â      â””â”€â”€ update_model.py
â”œâ”€â”€ imgs
â”‚Â Â  â””â”€â”€ architecture.png
â”œâ”€â”€ log  ## Log the test_loss and test_accuracy for models of different fed algorithms.
â”œâ”€â”€ models      ## Comprises model definitions for different datasets used in the project.
â”‚Â Â  â”œâ”€â”€ CIFAR10.py
â”‚Â Â  â”œâ”€â”€ FMNIST.py
â”‚Â Â  â”œâ”€â”€ get_model.py
â”‚Â Â  â””â”€â”€ MNIST.py
â”œâ”€â”€ privacy_analysis    ## Focused on the privacy analysis of the implemented algorithms, with scripts to compute and visualize privacy metrics.
â”‚Â Â  â”œâ”€â”€ log
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ privacy_loss_comparsion.png
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ privacy_loss_comparsion(q=0.01, sigma=1.0).png
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ privacy_loss_comparsion(sigma=1.0).png
â”‚Â Â  â”œâ”€â”€ compute_dp_sgd.py
â”‚Â Â  â”œâ”€â”€ compute_rdp.py
â”‚Â Â  â”œâ”€â”€ get_MaxSigma_or_MaxSteps.py
â”‚Â Â  â”œâ”€â”€ plot.py
â”‚Â Â  â”œâ”€â”€ rdp_convert_dp.py
â”‚Â Â  â”œâ”€â”€ shuffle.py
â”‚Â Â  â””â”€â”€ simulation_privacy_loss.py
â”œâ”€â”€ saved_states        ## Directory for storing saved model states or training checkpoints.
â”œâ”€â”€ train_and_validation    ## Contains scripts for model training and validation in a federated learning setting.
â”‚Â Â  â”œâ”€â”€ train.py
â”‚Â Â  â”œâ”€â”€ train_with_dp.py
â”‚Â Â  â””â”€â”€ validation.py
â”œâ”€â”€ utils
â”‚Â Â  â””â”€â”€ dp_optimizer.py
â”œâ”€â”€ config.yml          ## Project configuration settings.
â”œâ”€â”€ environment.yml     ## Specifies the project's environment requirements.
â”œâ”€â”€ LICENSE             ## Project's license file.
â”œâ”€â”€ main.py             ## The main entry point of the project.
â””â”€â”€ README.md           ## Documentation for the project.
```

## ğŸ³ Installation Guide

This project requires Python 3.9 or higher. Follow these steps to install the necessary dependencies:

```bash
git clone https://github.com/jjllzhang/DPFL4GraduateProject.git
cd DPFL4GraduateProject
conda env create -f environment.yml
conda activate DPFL
```

## ğŸ§© Usage Example

Here's a simple example of how to start a federated learning task with differential privacy and the Shuffler mechanism enabled:

- First, check the parameters in [`config.yml`](./config.yml).
- Modify the parameters as desired.
- Then, run the following command to start training the model:

```bash
python main.py
```

## ğŸ“Š Technical Architecture and Key Technologies

### ğŸ”’ Architecture Overview

The system architecture is designed to ensure privacy protection while maintaining efficiency in federated learning.

![The system architecture](./imgs/architecture.png)

### ğŸ” Differential Privacy

The project implements the following types of Differential Privacy Federated Learning:

- Per-layer DP
- Auto DP
- DP with Shuffler

Differential Privacy ensures that the removal or addition of a single database item does not significantly affect the outcome of any analysis, providing strong privacy guarantees for individuals' data.

### ğŸ”„ Shuffler Mechanism

The Shuffler mechanism adds an additional layer of privacy by randomly permuting data points, helping to break the link between the data and its source and further enhancing privacy.

### ğŸŒ Federated Learning

The core of the project, Federated Learning, is a distributed machine learning approach that enables multiple participants to collaboratively learn a shared model while keeping their data local.

## ğŸ’¡ Features

- **Privacy-Preserving**: Implements 3 kinds of Differential Privacy Federated Learning and Shuffler mechanisms.
- **Scalability**: Designed to efficiently handle large-scale federated learning tasks.
- **Flexibility**: Supports various federated learning scenarios and configurations.
- **Privacy-Accountant**: Uses Renyi Differential Privacy to account for privacy loss and get tighter bounds.

## ğŸ“ˆ Performance Metrics and Advantages

The system has been rigorously tested under different conditions, demonstrating significant improvements in privacy protection without compromising learning efficiency.

### âœ”ï¸ Test Accuracy Comparison

![Test loss comparison](./log/MNIST/test_accuracy_MNIST.png)

### ğŸ’¸ Privacy Budget Comparison

![Privacy budget comparison](./privacy_analysis/log/privacy_loss_comparsion.png)

## ğŸ“œ Copyright and License

This project is licensed under the Apache License - see the [`LICENSE`](./LICENSE) file for details.

## ğŸ™ Acknowledgments

- Special thanks to [@JeffffffFu](https://github.com/JeffffffFu) for his [DP videos](https://space.bilibili.com/80356866/video) on Bilibili and for his code implementation, which helped significantly.
- Gratitude to the lab for providing a server for experimental testing.
- Appreciation for the open-source community for the tools and libraries that made this project possible.
